# Integrating Natural Language Chat UIs

> A practical guide to using open source UI frameworks to build conversational experiences.
> The goal is to share a model that can be adapted across applications, while aligning with the manifesto‚Äôs principles of trustworthy engineering.

Natural language interfaces (NLIs) are changing the way organizations communicate with both prospective and existing customers. Instead of forcing users to navigate static menus or search through dense FAQs, conversational interfaces allow visitors to type (or speak) their questions directly and receive tailored, context-aware responses.

When implemented thoughtfully, NLIs can become powerful sales, marketing, and support aides. On public websites, they guide anonymous users to the right information, reducing friction in the buying journey. Inside authenticated portals, they become integral parts of customer service, support, and success operations‚Äîstreamlining workflows and improving satisfaction.

Natural language chat has become a defining way for users to interact with applications. Instead of rigid forms, dropdowns, or command-line interfaces, people now expect to communicate with software in a conversational way that feels intuitive, responsive, and human-like.

But designing these interactions is not just about embedding a chat bubble or connecting to a large language model (LLM). To create trustable AI interactions, we must ensure that the chat experience is transparent, respectful of user intent, protective of data, and reliable under real-world conditions.

## Natural Language Interfaces for Anonymous Website Visitors

When integrating NLIs into external, customer-facing websites, the goals differ from authenticated portals. Visitors are often anonymous, seeking information about products, services, or pricing. The chat assistant must act as a sales and marketing aide, not a support representative.

üëâ [This document](../ai-native-technical-communications-doctrine/embedding-ai-context-in-webpages.md) suggests ways to setup context for public web pages and is worth reading.

1. Context is everything: An NLI on a marketing site must be aware of the page the visitor is viewing. For example, if the visitor is reading about ‚Äúpricing,‚Äù the assistant should prioritize pricing-related responses.

2. Balance automation with transparency: Users should know they are interacting with an AI assistant, not a human. Transparency prevents disappointment and builds trust.

3. Capture interest without intrusiveness: The assistant should be proactive but not disruptive. For example, after a period of browsing, it can ask, ‚ÄúWould you like help understanding which plan is right for you?‚Äù

4. Pathways to conversion: The chat should not exist in isolation. It should be able to hand off to forms, schedule demos, or trigger callbacks from sales teams.

5. Data privacy: Since users are anonymous, the system must handle inputs without creating unnecessary profiles or storing personal information without consent.

### Example Interaction

* Visitor on the ‚ÄúFeatures‚Äù page:
  *User*: ‚ÄúDoes this integrate with Salesforce?‚Äù
  *Assistant*: ‚ÄúYes. Our integration with Salesforce allows leads to be synced automatically. Would you like me to show you a case study?‚Äù

## Natural Language Interfaces in Authenticated Portals

For logged-in customers, the NLI must shift roles‚Äîfrom a marketing aide to a support and service partner. These users have established relationships with the company and need quick, accurate access to account-specific information.

1. Respect authentication and authorization: The assistant must only expose data the user has permission to view. Role-based access controls are critical.

2. Tight integration with support systems: The NLI should connect with ticketing, knowledge bases, and support documentation. If a question cannot be answered immediately, the assistant should create a support case.

3. Consistent identity: Once authenticated, the assistant should use the customer‚Äôs profile for context. For example, ‚ÄúHi Alex, I see you are on the Enterprise plan. Do you want to know about advanced features?‚Äù

4. Escalation paths: AI assistants cannot and should not handle every scenario. There must be clear pathways for escalation to live support agents.

5. Personalized success guidance: Beyond troubleshooting, the assistant can act as a customer success aide‚Äîsuggesting new features, offering training resources, and sharing tailored tips.

### Example Interaction

* Authenticated customer in a portal:
  *User*: ‚ÄúHow do I add another user to my account?‚Äù
  *Assistant*: ‚ÄúYou can add team members under Settings ‚Üí Users. Would you like me to take you directly to that page?‚Äù

## Extending Enterprise Systems with Natural Language Interfaces

Traditionally, enterprise technology has been divided into three categories:

* Systems of Record: Databases, CRMs, ERPs where business data is stored and updated.
* Systems of Engagement: Interfaces where users interact with the organization (apps, websites, portals).
* Systems of Analytics: Dashboards and BI tools for reporting, forecasting, and decision-making.

Natural language interfaces can be integrated into all three categories, making them first-class citizens alongside CRUD operations and analytics queries.

### Systems of Record

* Traditional model: CRUD operations on structured data.
* NLI extension: Users can query records conversationally (‚ÄúShow me all open invoices for Acme Corp‚Äù) and even perform actions (‚ÄúCreate a new lead in Salesforce named Jane Doe‚Äù).
* Lessons learned: NLIs must map natural language into structured database operations reliably. Guardrails are required to prevent unauthorized updates.

### Systems of Engagement

* Traditional model: Forms, menus, dashboards.
* NLI extension: Chat assistants embedded into apps provide a conversational front-end to engagement workflows. For example, ‚ÄúBook me a demo for next Tuesday at 2 PM.‚Äù
* Lessons learned: NLIs reduce friction but should not replace all structured workflows. Instead, they should complement them.

### Systems of Analytics

* Traditional model: Predefined reports and dashboards.
* NLI extension: Users ask questions directly of their data. Example: ‚ÄúWhat were our top 5 products by revenue last quarter?‚Äù
* Lessons learned: Query translation must be accurate, and the assistant should show both the natural language answer and the underlying query (for transparency).

## Best Practices for Trustable Integration

Natural language interfaces can transform how organizations interact with both anonymous visitors and authenticated customers. On public websites, they act as sales and marketing aides, guiding prospects toward conversion. In customer portals, they become trusted support and success partners. And across enterprise technology stacks, they extend systems of record, engagement, and analytics into conversational, human-friendly experiences.

The lesson is clear: NLIs are not just a new user interface layer. They are becoming first-class participants in enterprise ecosystems, augmenting traditional CRUD operations, reports, and dashboards with conversational access. Implemented responsibly, they help organizations move toward systems that are not only more powerful, but also more human-centered and trustworthy.

1. Transparency: Always disclose when a user is interacting with AI.
2. Guardrails: Prevent unsafe or unauthorized operations, especially in systems of record.
3. Context-awareness: Tailor responses to the user‚Äôs location (anonymous vs. authenticated, marketing site vs. support portal).
4. Escalation: Provide clear handoffs to humans when the assistant cannot solve the problem.
5. Auditability: Ensure interactions are logged appropriately for compliance and improvement without violating user privacy.

## Conversational UX Integration Playbook

Before diving into specific playbook elements, keep these principles in mind for natural language interface (NLI) integration:

* Contextual awareness: Responses must reflect the environment ‚Äî website page, authenticated portal, or enterprise data domain.
* Transparency: Always disclose that the user is interacting with an AI assistant, and show when responses are grounded in documents or data.
* Reliability and speed: Always stream responses when possible, provide progress indicators, and degrade gracefully if services fail.
* Guardrails: Prevent unsafe or unauthorized operations, particularly in authenticated or system-of-record contexts.
* Human handoff: Always provide escalation paths to human agents.
* Observability: Log interactions responsibly to improve system reliability without violating user privacy.

### Anonymous Knowledge Experiences (Websites & Microsites)

These are the entry points for anonymous, unauthenticated users who are exploring products or services. The assistant here plays the role of a sales and marketing aide.

#### UX Expectations

* The assistant should understand the page context (e.g., ‚ÄúPricing page‚Äù vs. ‚ÄúFeatures page‚Äù) so it can ground responses in what the user is currently viewing.
* Provide starter suggestions like ‚ÄúCompare plans‚Äù or ‚ÄúShow integrations‚Äù to reduce friction.
* Responses should link to canonical sources on the site, so users can confirm answers.
* Avoid intrusive prompts; assistants should be helpful, not pushy.

#### Backend Responsibilities

* Capture and pass page metadata (URL, title, product category) with the user‚Äôs query.
* Retrieve relevant website content using an index of pages and documents.
* Ensure the assistant does not invent pricing or SLAs. Instead, direct users to official pricing pages.
* Responses must cite timestamps and source links so users know when and where the information came from.

#### What the Code Should Include

* Logic to inject page context (URL, product tags, section headers) into the AI prompt.
* A retrieval mechanism that fetches the most relevant website snippets to ground responses.
* A response assembly step that combines the AI‚Äôs answer with references and links.
* A policy filter that blocks sensitive or irrelevant content from being surfaced.

### Authenticated Portal Experiences (Support, Service, Success)

For authenticated users, the assistant shifts roles to act as a support and service partner. These users expect quick, accurate, account-specific information.

#### UX Expectations

* The assistant should greet users with contextual awareness of their account or plan (‚ÄúYou‚Äôre on Enterprise‚Äù).
* Offer shortcuts to common actions like ‚ÄúView invoices‚Äù or ‚ÄúCheck ticket status.‚Äù
* Provide confirmation dialogs before performing any state-changing operations like updating account details.
* Responses should include citations (knowledge base articles, past tickets, product docs).
* Support seamless handoff to human agents when needed.

#### Backend Responsibilities

* Authenticate and authorize every request using JWTs or session tokens.
* Enforce role-based access controls when calling internal tools or APIs.
* Connect to enterprise systems like ticketing, billing, and knowledge bases.
* Log actions in an auditable way for compliance, while avoiding unnecessary storage of personal data.

#### What the Code Should Include

* Middleware that checks user identity and permissions before any tool or API call.
* A policy engine that blocks requests outside of user entitlements.
* Secure adapters to support systems (ticketing, billing, product usage).
* A confirmation pattern where the assistant proposes an action, and the user must explicitly approve it before execution.
* Logic for handoff to human agents that includes a scrubbed summary of the conversation.

### B2B Systems of Record and Systems of Analytics

Here, the assistant becomes a first-class operator within enterprise workflows. This is where NLIs must be handled with the greatest care because they interact with structured data and business-critical operations.

#### UX Expectations

* For systems of record (CRMs, ERPs): Users should be able to query (‚ÄúShow all open opportunities for Acme Corp‚Äù) and propose updates (‚ÄúCreate a new lead named Jane Doe‚Äù).
* For systems of analytics (dashboards, BI tools): Users should ask natural language questions (‚ÄúWhat were top five products by revenue last quarter?‚Äù) and receive both plain-language answers and the underlying query for transparency.
* Responses must always show their work ‚Äî e.g., SQL query used, filters applied, or source tables accessed.
* Provide undo or rollback options for state-changing operations.

#### Backend Responsibilities

* Map natural language requests to structured queries or tool calls reliably.
* Validate all parameters and apply safety checks before committing any changes.
* Restrict the assistant to read-only operations at first, then introduce writes with confirmations and approvals.
* For analytics, connect to a semantic layer or governed schema to ensure queries are meaningful and compliant.
* Log all actions with lineage information: which records or tables were accessed, which fields were updated.

#### What the Code Should Include

* Tool schemas that define what operations are allowed, with required parameters and validations.
* Guardrails that block unsafe or destructive operations (e.g., no deletes without admin approval).
* A confirmation workflow: assistant proposes ‚Üí user reviews ‚Üí system executes.
* Audit trails that record what queries were generated and what results were returned.
* A transparency layer for analytics that shows the generated SQL alongside the summarized results.

### Observability and Continuous Improvement

Natural language interfaces must be continuously monitored and improved.

#### Metrics to Track

* Latency: time to first token, time to full response.
* Accuracy: percentage of answers grounded in valid sources.
* Deflection: how many questions resolved without human intervention.
* Escalations: when and why users are handed off to humans.
* Retrieval quality: whether relevant documents are surfaced correctly.

#### Improvement Loops

* Maintain golden datasets of representative questions and expected answers for regression testing.
* Track content gaps: queries that fail repeatedly should drive updates to knowledge bases.
* Test prompt variations and tool configurations in A/B experiments.

### Rollout Strategy

* Websites: Start with read-only Q\&A grounded in site content. Expand later to offer lead capture or demo scheduling with user consent.
* Portals: Begin with FAQs and ticket creation. Gradually extend to billing lookups and limited account updates with confirmation.
* Systems of record/analytics: Start read-only. Add low-risk writes (create lead, add note). Expand cautiously to updates and deletes with strict role checks and audit trails.

Natural language interfaces can transform customer-facing websites, authenticated portals, and enterprise systems into more human-centered experiences. For anonymous users, they reduce friction in sales and marketing journeys. For authenticated customers, they streamline support and success workflows. For B2B systems, they extend CRUD and analytics into conversational access layers.

The engineering challenge is not just connecting an LLM to a chat window. It is building a trustable integration with clear UX patterns, strong guardrails, transparent operations, and reliable backend services. When implemented carefully, NLIs become first-class citizens of enterprise ecosystems, standing alongside traditional dashboards, forms, and APIs.

## Choosing an Open Source UI Framework

The first question teams face is: *Which UI framework should we use for chat?*

A trustworthy choice must balance technical quality with community reliability. The following criteria are essential:

* Active community: Does the project have ongoing development, issue tracking, and contributor support?
* Extensibility: Can it be customized to support enterprise workflows and design patterns?
* Compatibility: Does it integrate smoothly with modern LLM backends and APIs?
* Transparency: Is the code open and auditable, ensuring security and alignment with trustable AI principles?

One strong option is [`assistant-ai`](https://github.com/assistant-ai), an open source framework that provides a ready-made chat UI, session management, and conversation history. It is modular, developer-friendly, and directly applicable to the manifesto‚Äôs goal of transparent, controllable, and auditable AI interactions.

## Setting Up the Chat UI

The first step in building a trustable natural language interface is to set up a reliable and flexible chat UI. Open source frameworks like `assistant-ai` provide a ready-to-use foundation that can be adapted for different applications. The `assistant-ai` package can be added to a project using standard dependency managers (e.g., `npm` or `yarn` in JavaScript projects). Once installed, it provides reusable UI components such as the chat window, message bubbles, and conversation controls.

```bash
npm install assistant-ai
```

After installation, the chat UI needs to be connected to an AI backend. This is typically an API endpoint that accepts user input and returns model-generated responses. Configuration is handled through environment variables or direct parameters in the UI component.

Example (React integration):

```jsx
<AssistantChat 
  apiEndpoint="https://api.mybackend.com/chat"
  userSessionId={currentUser.id}
/>
```

In this setup:

* `apiEndpoint` points to the server responsible for handling the LLM conversation.
* `userSessionId` ensures that each user‚Äôs messages are tied to a consistent conversation history.

### Managing Sessions and Conversation History

A key requirement for trustable interactions is maintaining continuity. `assistant-ai` provides hooks for persisting messages locally or in a database, allowing users to return to prior conversations. This improves user confidence by ensuring context is not lost between sessions.

Example considerations:

* Store messages in a secure backend linked to the user‚Äôs account.
* Implement timeouts or conversation resets for privacy and clarity.
* Provide users with the ability to delete or export their history.

### Error Handling

To reinforce reliability, the chat UI should include safeguards against API failures or unexpected responses. This includes:

* Displaying a friendly error message when the backend is unreachable.
* Offering retry options to the user.
* Logging errors for developers without exposing technical details to the end user.

By following these setup steps, teams can quickly stand up a functioning, user-friendly chat interface that aligns with the manifesto‚Äôs principles of transparency, reliability, and user control.

## Integrating Natural Language Capabilities

A chat UI is only as useful as the intelligence it connects to. Integrating natural language capabilities involves linking the UI with a backend that can process user queries and return AI-generated responses.

The workflow typically looks like this:

1. User input ‚Üí typed or spoken in the chat window.
2. Frontend capture ‚Üí message passed to the backend via the `apiEndpoint`.
3. Backend processing ‚Üí message is routed to an LLM (e.g., via OpenAI, Anthropic, or a local model).
4. Response streaming ‚Üí model‚Äôs output is streamed back in real time.
5. UI rendering ‚Üí assistant‚Äôs response displayed with typing indicators and final text.

For Opsfolio.com, we implemented this flow using a secure API gateway, ensuring each message was authenticated and logged. This allowed enterprise users to rely on the chat while IT administrators retained visibility into the system‚Äôs behavior.

## Enhancing User Experience

Trustable AI interactions go beyond functionality ‚Äî they must also *feel* reliable and supportive. The chat UI should incorporate patterns that reinforce clarity and confidence:

* Typing indicators to show progress and reduce uncertainty.
* Visual distinction between user, system, and assistant messages.
* Clear error messages that respect user attention and avoid jargon.
* Accessibility features such as keyboard navigation and screen reader support.

A good user experience creates a sense of partnership between the human and the AI, making the system feel approachable while avoiding over-promising or misleading behavior.

## Extending Functionality

A chat interface should not be static. Open source frameworks like `assistant-ai` can be extended to include:

* Custom plugins for domain-specific tasks (e.g., retrieving enterprise data, automating workflows).
* Integration with workflows such as incident management, reporting, or task tracking (as done in Opsfolio).
* Future modalities such as voice input, visual annotations, or multi-agent collaboration.

This flexibility ensures that the chat interface grows with user needs rather than becoming a dead-end tool.

## Conversational UX Implementation Strategy

### Landscape View (who talks to whom)

```
[Anonymous Visitor]                      [Authenticated User]                  [Internal Staff]
        |                                          |                                  |
        v                                          v                                  v
+-------------------+                       +-------------------+               +-------------------+
|  Website/Microsite|                       |   Customer Portal |               |  Internal Console |
|  (Public Surface) |                       |  (AuthN + RBAC)   |               |   (Ops/CS/CX)     |
+---------+---------+                       +---------+---------+               +---------+---------+
          |                                           |                                   |
          | page ctx, query                           | user claims, query                | admin queries
          v                                           v                                   v
                           +---------------------------------------------------+
                           |              API Gateway / Edge Proxy             |
                           |  (rate limit, WAF, TLS, tenant routing, PII scrub)|
                           +---------------------+-----------------------------+
                                                 |
                                                 v
                                    +---------------------------+
                                    |   NL Orchestrator         |
                                    | (prompting, tools, RAG,   |
                                    |  policy, auditing)        |
                                    +------------+--------------+
                                                 |
     +----------------------+--------------------+-----------------------+
     |                      |                    |                       |
     v                      v                    v                       v
+-----------+       +---------------+    +---------------+        +------------------+
| Vector/RAG|       |  SoR Adapters |    | Analytics     |        |  LLM Providers   |
| (docs, KB)|       | (CRM/ERP/CS)  |    | Adapters      |        | (primary/fallback|
+-----+-----+       +-------+-------+    | (SQL/semantic)|        |   + guardrails)  |
      |                     |            +-------+-------+        +------------------+
      |                     |                    | 
      v                     v                    v
[Docs CMS/Web]      [Systems of Record]   [Warehouse/BI/Semantic]
```

Intent: three front-doors (public site, portal, internal console) share one secure API Gateway and a policy-driven NL Orchestrator. The orchestrator is the ‚Äúbrain‚Äù that composes prompts, runs retrieval, calls tools, enforces policy, and audits.

### Request Lifecycle (step-by-step sequence)

```
1) Client ‚Üí Gateway
   - Website: {query, page_url, page_title}
   - Portal:  {query} + JWT (user_id, org_id, roles)
   - Console: {query} + staff JWT

2) Gateway
   - Terminate TLS, apply WAF/rate limits
   - Scrub obvious PII in public context
   - Forward with tenant/region hints

3) Orchestrator
   - Classify intent (marketing Q, support Q, CRUD, analytics)
   - Build context: page metadata OR user entitlements + recent activity
   - Retrieve: vector search + keyword fallback; re-rank top passages
   - Assemble prompt: role + policies (grounding, no price invention, confirm-before-commit)
   - Plan tools: which adapters might be needed (e.g., ticketing, invoices, SQL)

4) Policy Engine (pre-call)
   - Check RBAC/ABAC: is this user allowed to access this tool/data?
   - Drop/deny unsafe actions in public context (e.g., account lookups)

5) LLM (streaming)
   - Generate with citations and action proposals (not commits)
   - If tool call proposed ‚Üí return structured call

6) Tool Execution (server-side)
   - Validate inputs; enforce invariants and tenant boundaries
   - Execute read-only first; for writes show a confirmation card pattern
   - Return minimal results for privacy; attach lineage/links

7) LLM (finalize)
   - Compose grounded answer + citations + next best actions
   - For analytics: include natural-language summary + ‚ÄúSee underlying query‚Äù metadata
   - For SoR changes: include ‚ÄúPreview‚Äù vs ‚ÄúConfirm‚Äù branches

8) Gateway ‚Üí Client (stream)
   - Typing indicator ‚Üí partial chunks ‚Üí final message
   - Optional ‚ÄúView sources,‚Äù ‚ÄúOpen ticket,‚Äù ‚ÄúBook demo‚Äù actions

9) Observability (async)
   - Emit trace (IDs only), counters (latency, hit/miss), and redacted logs
   - Store evaluation hooks for offline quality checks
```

### Public Website/Microsite Assistant (anonymous)

Experience goals

* Page-aware answers: reflect the section the user is reading (e.g., Pricing vs. Security).
* Trust-by-default: show sources + last updated timestamps; never invent SLAs or pricing.
* Soft conversion: optional ‚ÄúEmail me this,‚Äù ‚ÄúCompare plans,‚Äù ‚ÄúBook a demo‚Äù (explicit consent).

What implementation must include

* Capture page context (URL/title/taxonomy) with each query.
* Retrieval that prefers on-page and closely related pages; freshness-aware.
* A policy layer that blocks sensitive topics (account data) and price fabrication.
* Response assembler that always attaches citations and canonical links.
* Metrics: time-to-first-token, deflection from ‚ÄúContact Sales,‚Äù % answers with ‚â•2 sources.

### Authenticated Portal Assistant (support, service, success)

Experience goals

* Personalized, entitlement-aware help: ‚ÄúYou‚Äôre on Enterprise; your last invoice is due on‚Ä¶‚Äù
* Action models with confirmation: propose ‚Üí preview ‚Üí confirm to execute changes.
* Seamless escalation: one-click handoff to a human with a scrubbed context pack.

What implementation must include

* AuthN/Z: JWT with user/org/roles; server-side RBAC on every tool call.
* Secure adapters for ticketing, billing, product usage, and tenant KB.
* Confirm-before-commit pattern for any write (updates, cancellations, key rotations).
* ‚ÄúWhy this answer?‚Äù panel with sources, data freshness, and policy statements.
* Metrics: first-contact resolution, tool success rate, escalation reasons, RBAC denials (should be rare).

### B2B Systems of Record (CRUD) Assistant

Experience goals

* Safe operator: clear previews, reversible changes, and complete audit trails.
* Precision over creativity: constrained operations with explicit parameters.

What implementation must include

* Tool contracts defining each allowed operation, required fields, and validations.
* Server-side guards for ownership, invariants, and multi-tenant isolation.
* Dual confirmation flows for risky actions; optional supervisor approval.
* Audit trails: who requested, who confirmed, what changed, before/after snapshots.
* Rollout: read-only ‚Üí low-risk creates (leads, notes) ‚Üí updates/deletes behind stricter gates.

### Systems of Analytics Assistant

Experience goals

* Natural questions with transparent answers: show both summary and the underlying query.
* Governed access: only permitted schemas/tables; no accidental data sprawl.

What implementation must include

* A semantic layer or governed schema the assistant is allowed to query.
* Query transparency: expose generated query (e.g., ‚ÄúView query‚Äù) and filters used.
* Cost/impact checks for long/expensive queries; user confirmation before execution.
* Data freshness notices; offer ‚ÄúRefresh with latest partition.‚Äù
* Lineage logging: tables used, model version, and result cache indicators.

### Content & Retrieval Architecture (for grounded answers)

Experience goals

* Answers that cite source passages, not vague claims.
* Fresh content reflected quickly after site or KB updates.

What implementation must include

* Content pipeline that chunks pages/docs (keep headings, drop boilerplate).
* Hybrid retrieval: dense vectors + keyword fallback + re-ranking.
* Freshness strategies: scheduled recrawls + CMS webhooks to re-index.
* Answer composer that quotes short excerpts and deep-links to anchors.
* Monitoring: retrieval hit rate, passage usefulness, stale-content alerts.

### Policy, Privacy, and Compliance

Experience goals

* Respect user privacy by default; explicit consent for any PII capture.
* Regional compliance and tenant isolation without friction.

What implementation must include

* Data minimization in public flows; redaction of PII in telemetry.
* Retention windows: shorter for public, governed for portals (with DSR support).
* Regional pinning of indices/logs when required (e.g., EU-only processing).
* Vendor controls: disable model training on your data; DPAs in place.
* Clear user-facing AI disclosure and ‚Äúopt-out of experiments‚Äù where applicable.

### Observability, QA, and Continuous Improvement

Experience goals

* Predictable performance and measurable quality.
* Continuous reduction of hallucinations and retrieval misses.

What implementation must include

* Distributed tracing per turn (client ‚Üí gateway ‚Üí orchestrator ‚Üí tools ‚Üí LLM).
* SLIs/SLOs: p50/p95 latency, grounded-answer rate, tool error rate, escalation rate.
* Golden test sets per surface (public, portal, analytics) for regression gates.
* Offline evaluations: groundedness, safety, SQL sanity checks.
* A/B infrastructure for prompt/tool variants; log outcomes, not raw text.

### Deployment & Rollout Patterns

Public website

* Phase 1: Read-only, fully grounded answers with sources; no lead capture.
* Phase 2: Add consented CTAs (email summary, demo booking); measure conversion uplift.
* Phase 3: Product finder, plan comparison; continuous content gap filling.

Portal

* Phase 1: FAQs + ticket creation; strong RBAC and confirmations.
* Phase 2: Billing lookups, invoice downloads, entitlement checks.
* Phase 3: Carefully gated account changes with dual confirmation and audit.

B2B CRUD/Analytics

* Phase 1: Read-only lookups and analytics explanations with query transparency.
* Phase 2: Low-risk creates (leads, notes) with approvals.
* Phase 3: Updates/deletes with role gates, approvals, and rollback paths.

### ‚ÄúWhat the Code Should Have‚Äù ‚Äî One-Page Checklist

* Client: page/app context injection; streaming UX; citations panel; consented CTAs; confirmation modals.
* Gateway: TLS, WAF, rate limits, region routing, PII scrubbing, metrics.
* Orchestrator: intent classification; prompt assembly; retrieval; tool planning; policy checks; audit hooks.
* Tools/Adapters: narrow, validated contracts; tenant isolation; idempotency; timeouts; structured errors.
* Policy Engine: RBAC/ABAC; surface-based restrictions; deny-lists; confirmation-before-commit.
* RAG: hybrid search; freshness; re-ranking; snippet builder; link anchors.
* Analytics: governed schema; transparency of queries; cost guards; freshness notices.
* Observability: tracing, metrics, redacted logs, evaluation harness, A/B testing.
* Privacy/Compliance: data minimization, retention policies, regional pinning, DSR tooling, model data controls.

